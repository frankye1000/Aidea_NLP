{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41411</td>\n",
       "      <td>I watched this film because I'm a big fan of R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37586</td>\n",
       "      <td>It does not seem that this movie managed to pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6017</td>\n",
       "      <td>Enough is not a bad movie , just mediocre .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44656</td>\n",
       "      <td>my friend and i rented this one a few nights a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38711</td>\n",
       "      <td>Just about everything in this movie is wrong, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                             review  sentiment\n",
       "0  41411  I watched this film because I'm a big fan of R...          0\n",
       "1  37586  It does not seem that this movie managed to pl...          1\n",
       "2   6017        Enough is not a bad movie , just mediocre .          0\n",
       "3  44656  my friend and i rented this one a few nights a...          0\n",
       "4  38711  Just about everything in this movie is wrong, ...          0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "from string import punctuation\n",
    "\n",
    "ls   = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "ls[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 清除標點符號、切割小寫、stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remove_chars = '[0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'\n",
    "\n",
    "Sentense = []\n",
    "for r in ls.review:\n",
    "    string = re.sub(remove_chars, \" \",r)                                                         # 去除標點符號\n",
    "    tokens = nltk.word_tokenize(string.lower())                                                  # 句子切割和小寫\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens if t not in stops]                 # 去除stopword，lemmatize則是字根化\n",
    "    Sentense.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 測試集資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_Sentense = []\n",
    "for r in test.review:\n",
    "    string = re.sub(remove_chars, \" \",r)                                                         # 去除標點符號\n",
    "    tokens = nltk.word_tokenize(string.lower())                                                  # 句子切割和小寫\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens if t not in stops]                 # 去除stopword，lemmatize則是字根化\n",
    "    T_Sentense.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 128     #每個字的向量維度\n",
    "model = word2vec.Word2Vec(Sentense+T_Sentense, min_count=1, vector_size=vector_size, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for se in Sentense:\n",
    "    zero = np.zeros(vector_size)\n",
    "    for word in se:\n",
    "        zero += model.wv[word]\n",
    "    \n",
    "    X.append(zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(ls.sentiment)\n",
    "len(Y)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 貝氏分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB, MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "gnb = LinearSVC()\n",
    "\n",
    "#Train the model using the training sets\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 測試集結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = []\n",
    "for se in T_Sentense:\n",
    "    zero = np.zeros(vector_size)\n",
    "    for word in se:\n",
    "        zero += model.wv[word]\n",
    "    Z.append(zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = scaler.fit_transform(Z)\n",
    "result = gnb.predict(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"ID\":test.ID,\"sentiment\":result}).to_csv(\"result.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 想測試TFIDF  \n",
    "### 維度會太大，須限制維度\n",
    "### TFIDF完，維度就太大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "remove_chars = '[0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'\n",
    "\n",
    "Sentense = []\n",
    "for r in ls.review:\n",
    "    s = \" \"\n",
    "    string = re.sub(remove_chars, \" \", r)                                                         # 去除標點符號\n",
    "    tokens = nltk.word_tokenize(string.lower())                                                  # 句子切割和小寫\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens if t not in stops]                 # 去除stopword，lemmatize則是字根化\n",
    "    corpus = s.join(tokens)\n",
    "    Sentense.append(corpus)\n",
    "\n",
    "\n",
    "T_Sentense = []\n",
    "for r in test.review:\n",
    "    t = \" \"\n",
    "    string = re.sub(remove_chars, \" \",r)                                                         # 去除標點符號\n",
    "    tokens = nltk.word_tokenize(string.lower())                                                  # 句子切割和小寫\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens if t not in stops]                 # 去除stopword，lemmatize則是字根化\n",
    "    corpus = t.join(tokens)\n",
    "    T_Sentense.append(corpus)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(Sentense+T_Sentense)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29341\n",
      "29341\n"
     ]
    }
   ],
   "source": [
    "Z = X.toarray()[29341:]\n",
    "X = X.toarray()[:29341]\n",
    "Y = np.array(ls.sentiment)\n",
    "print(len(X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB, MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "gnb = LinearSVC()\n",
    "\n",
    "#Train the model using the training sets\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7946154719981824\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = scaler.fit_transform(Z)\n",
    "result = gnb.predict(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"ID\":test.ID,\"sentiment\":result}).to_csv(\"result.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier(\n",
    "        #樹的個數\n",
    "        n_estimators=100,\n",
    "        # 如同學習率\n",
    "        learning_rate= 0.3, \n",
    "        # 構建樹的深度，越大越容易過擬合    \n",
    "        max_depth=6, \n",
    "        # 隨機取樣訓練樣本 訓練例項的子取樣比\n",
    "        subsample=1, \n",
    "        # 用於控制是否後剪枝的引數,越大越保守，一般0.1、0.2這樣子\n",
    "        gamma=0, \n",
    "        # 控制模型複雜度的權重值的L2正則化項引數，引數越大，模型越不容易過擬合。\n",
    "        reg_lambda=1,  \n",
    "        \n",
    "        #最大增量步長，我們允許每個樹的權重估計。\n",
    "        max_delta_step=0,\n",
    "        # 生成樹時進行的列取樣 \n",
    "        colsample_bytree=1, \n",
    "\n",
    "        # 這個引數預設是 1，是每個葉子裡面 h 的和至少是多少，對正負樣本不均衡時的 0-1 分類而言\n",
    "        # 假設 h 在 0.01 附近，min_child_weight 為 1 意味著葉子節點中最少需要包含 100 個樣本。\n",
    "        #這個引數非常影響結果，控制葉子節點中二階導的和的最小值，該引數值越小，越容易 overfitting。\n",
    "        min_child_weight=1, \n",
    "\n",
    "        #隨機種子\n",
    "        seed=1000 \n",
    "        \n",
    "        # L1 正則項引數\n",
    "#        reg_alpha=0,\n",
    "        \n",
    "        #如果取值大於0的話，在類別樣本不平衡的情況下有助於快速收斂。平衡正負權重\n",
    "        #scale_pos_weight=1,\n",
    "        \n",
    "        #多分類的問題 指定學習任務和相應的學習目標\n",
    "        #objective= 'multi:softmax', \n",
    "        \n",
    "        # 類別數，多分類與 multisoftmax 並用\n",
    "        #num_class=10,\n",
    "        \n",
    "        # 設定成1則沒有執行資訊輸出，最好是設定為0.是否在執行升級時列印訊息。\n",
    "#        silent=0 ,\n",
    "        # cpu 執行緒數 預設最大\n",
    "#        nthread=4,\n",
    "    \n",
    "        #eval_metric= 'auc'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_features=4, random_state=0)\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    LinearSVC(random_state=0, tol=1e-5))\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
